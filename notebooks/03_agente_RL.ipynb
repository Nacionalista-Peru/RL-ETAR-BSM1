{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMqlJIWKDFzz"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento de Agente RL\n",
        "# Este notebook prepara y entrena un agente de aprendizaje por refuerzo (PPO/DQN) usando la simulación del BSM1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento de Agente RL\n",
        "# Este notebook prepara y entrena un agente de aprendizaje por refuerzo (PPO) usando la simulación del BSM1 (modo demostración).\n",
        "\n",
        "# Instalar librerías necesarias\n",
        "!pip install stable-baselines3[extra] optuna -q\n",
        "\n",
        "# Imports\n",
        "import gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from gym import spaces\n",
        "\n",
        "# Simulación simplificada del BSM1 (mock)\n",
        "class BSM1Env(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BSM1Env, self).__init__()\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)  # Control de oxígeno, por ejemplo\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(4,), dtype=np.float32)  # Ejemplo: DQO, NH4, NO3, SRT\n",
        "        self.state = np.array([50.0, 10.0, 5.0, 8.0])\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([50.0, 10.0, 5.0, 8.0])\n",
        "        self.t = 0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        oxigeno = action[0]\n",
        "        self.state += np.random.normal(0, 0.5, size=self.state.shape) - oxigeno\n",
        "        reward = -np.sum(np.square(self.state - np.array([30, 5, 2, 5])))  # Penaliza desviación del estado deseado\n",
        "        self.t += 1\n",
        "        done = self.t >= 96  # 96 pasos = 4 días si cada paso es 1h\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        print(f\"Step {self.t}, State: {self.state}\")\n",
        "\n",
        "# Crear y verificar el entorno\n",
        "env = BSM1Env()\n",
        "check_env(env)\n",
        "\n",
        "# Entrenar el agente PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=5000)\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save(\"ppo_bsm1_agent\")\n",
        "\n",
        "# Probar el agente entrenado\n",
        "obs = env.reset()\n",
        "for i in range(24):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, rewards, done, info = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "wET5AGnGJQTQ",
        "outputId": "85a66f47-92ba-4274-b465-83bcb0c475a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/383.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m378.9/383.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8548014fb8cf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Crear y verificar el entorno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBSM1Env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mcheck_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Entrenar el agente PPO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/env_checker.py\u001b[0m in \u001b[0;36mcheck_env\u001b[0;34m(env, warn, skip_render_check)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mTrue\u001b[0m \u001b[0mby\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0museful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \"\"\"\n\u001b[0;32m--> 430\u001b[0;31m     assert isinstance(\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     ), \"Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\"\n",
            "\u001b[0;31mAssertionError\u001b[0m: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar librerías necesarias\n",
        "!pip install gymnasium stable-baselines3[extra] optuna -q\n",
        "\n",
        "# Imports\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from gymnasium import spaces\n",
        "\n",
        "# Simulación simplificada del BSM1 (mock)\n",
        "class BSM1Env(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BSM1Env, self).__init__()\n",
        "        self.action_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)  # Control de oxígeno\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(4,), dtype=np.float32)  # DQO, NH4, NO3, SRT\n",
        "        self.state = np.array([50.0, 10.0, 5.0, 8.0], dtype=np.float32)\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.state = np.array([50.0, 10.0, 5.0, 8.0], dtype=np.float32)\n",
        "        self.t = 0\n",
        "        return self.state, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        oxigeno = action[0]\n",
        "        self.state += np.random.normal(0, 0.5, size=self.state.shape) - oxigeno\n",
        "        reward = -np.sum(np.square(self.state - np.array([30, 5, 2, 5])))  # Penaliza desviación del estado deseado\n",
        "        self.t += 1\n",
        "        done = self.t >= 96\n",
        "        return self.state, reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        print(f\"Step {self.t}, State: {self.state}\")\n",
        "\n",
        "# Crear y verificar el entorno\n",
        "env = BSM1Env()\n",
        "check_env(env)\n",
        "\n",
        "# Entrenar el agente PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=5000)\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save(\"ppo_bsm1_agent\")\n",
        "\n",
        "# Probar el agente entrenado\n",
        "obs, _ = env.reset()\n",
        "for i in range(24):\n",
        "    action, _states = model.predict(obs)\n",
        "    obs, reward, done, truncated, info = env.step(action)\n",
        "    env.render()\n",
        "    if done:\n",
        "        break"
      ],
      "metadata": {
        "id": "SnstKNwjLDDo",
        "outputId": "af7ebfe4-d9fc-4835-b533-f7157c4c1d81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/env_checker.py:462: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 96       |\n",
            "|    ep_rew_mean     | -6.5e+04 |\n",
            "| time/              |          |\n",
            "|    fps             | 1296     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 96           |\n",
            "|    ep_rew_mean          | -7e+04       |\n",
            "| time/                   |              |\n",
            "|    fps                  | 900          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019337073 |\n",
            "|    clip_fraction        | 0.00107      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.42        |\n",
            "|    explained_variance   | -4.18e-05    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.56e+07     |\n",
            "|    n_updates            | 10           |\n",
            "|    policy_gradient_loss | -0.00176     |\n",
            "|    std                  | 1            |\n",
            "|    value_loss           | 1.39e+08     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 96            |\n",
            "|    ep_rew_mean          | -6.91e+04     |\n",
            "| time/                   |               |\n",
            "|    fps                  | 836           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 7             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00075913407 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.42         |\n",
            "|    explained_variance   | 1.91e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.07e+08      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -0.000956     |\n",
            "|    std                  | 0.999         |\n",
            "|    value_loss           | 2.05e+08      |\n",
            "-------------------------------------------\n",
            "Step 1, State: [50.432693  10.0175495  4.977101   8.123915 ]\n",
            "Step 2, State: [49.618893   9.72254    4.7604647  8.211906 ]\n",
            "Step 3, State: [47.89604    8.721679   4.2134786  7.0169606]\n",
            "Step 4, State: [48.348534  9.366793  4.14302   7.413702]\n",
            "Step 5, State: [47.636448  8.974734  2.918921  5.794191]\n",
            "Step 6, State: [47.6792     8.8018055  3.269656   6.0908785]\n",
            "Step 7, State: [47.599323   8.443108   3.2705812  5.76251  ]\n",
            "Step 8, State: [47.011612   8.797873   3.1121612  6.044566 ]\n",
            "Step 9, State: [45.28177    7.87244    1.7437543  5.2208424]\n",
            "Step 10, State: [44.63887    8.584986   1.6196555  5.0519195]\n",
            "Step 11, State: [44.83       7.9671793  0.985148   5.920365 ]\n",
            "Step 12, State: [44.029186    7.290406   -0.58445364  4.963682  ]\n",
            "Step 13, State: [44.216316   7.4329967 -0.9418396  5.829217 ]\n",
            "Step 14, State: [43.822758   7.4937    -1.3209602  5.892108 ]\n",
            "Step 15, State: [44.421265   7.4609156 -1.3319628  5.4412794]\n",
            "Step 16, State: [43.203644   6.6392255 -2.2490952  5.235817 ]\n",
            "Step 17, State: [41.686947   4.714002  -3.2586834  3.303616 ]\n",
            "Step 18, State: [40.457848   4.84392   -3.9783552  2.7767034]\n",
            "Step 19, State: [40.781628   5.017799  -3.2643902  1.9461195]\n",
            "Step 20, State: [40.901207   5.314477  -3.2685916  2.260863 ]\n",
            "Step 21, State: [40.4862     5.720083  -3.8823678  2.737431 ]\n",
            "Step 22, State: [40.818295   5.700356  -4.70043    1.9423852]\n",
            "Step 23, State: [40.44303    5.227766  -4.7856197  2.6804059]\n",
            "Step 24, State: [39.00448    4.535071  -5.259359   1.1575273]\n"
          ]
        }
      ]
    }
  ]
}